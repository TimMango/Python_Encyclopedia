{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "<pre></pre>\n",
    "<font size=+1>\n",
    "    \n",
    "[Import Data](#Import-Data)<pre></pre>\n",
    "[Manipulating data](#Manipulating-date)<pre></pre>\n",
    "[Merging Dataframes and Groupby](#Merging-Dataframes-and-Groupby)<pre></pre>\n",
    "[Plots](#Plots)<pre></pre>\n",
    "[Imputing Data](#Imputing-Data)<pre></pre>\n",
    "[API Requests](#API-Requests)<pre></pre>\n",
    "[Logistic Regression Example](#Logistic-Regression-Example)<pre></pre>\n",
    "[Dictionaries Example](#Dictionaries-Example)<pre></pre>\n",
    "[Classes and OOP](#Classes-and-OOP)<pre></pre>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages usually want to upload with display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data with encoding type and explicit column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Csv_File_Name.csv', encoding='Encoding_Type',  names = ['Index', 'Column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get basic Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check values of a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for NA values, education has no missing values\n",
    "df_clean['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_fips_sliced_df['Full FIPS'] = string_fips_sliced_df['Full FIPS'].astype(dtype = int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Column_Name'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows the percent missing values for each  varialble in a dataframe\n",
    "\n",
    "DF_Percent_Missing = DF_Final_Vars.isna().mean().round(4)\n",
    "\n",
    "#This shows the variables with highest missing percent missing values first\n",
    "\n",
    "DF_Percent_Missing.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes all cells named 'Cell_ Value' to null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace \"unknown\" values as missing\n",
    "\n",
    "df = df_unclean.replace('unknown', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check counts of missing values\n",
    "#We decided to drop contact and poutcome from our analysis\n",
    "\n",
    "df_unclean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New column 1/1000th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to provide more manageable values, 'price was divided by 1000, \n",
    "# converting 'price' to thousands ($)\n",
    "df['price1000'] = df['price']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates new dataframe for all houses worth less than 1 million dollars\n",
    "\n",
    "df_Mill = df.loc[df['price'] < 1000000]\n",
    "df_Mill.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have a list of column names you would like to keep (If there are many many variables)\n",
    "#This creates a dataframe of the variables of interest\n",
    "\n",
    "Final_DF = df[List_Of_Kept_Var_Names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset of a dataframe\n",
    "# 1,000,000 ie 'price' would include only homes valued at under 1 million dollars\n",
    "df_Mill = df.loc[df['price'] < 1000000]\n",
    "df_Mill.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to string take slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Target FIPs codes ready to merge\n",
    "#data type needs to be string for manipulation\n",
    "string_fips = TRI_df['FIPS'].astype(dtype = str)\n",
    "\n",
    "#This reduces fips code to the block group level\n",
    "string_fips_sliced = string_fips.str.slice(0,-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This removes non numerical values from strings\n",
    "#Cleaned strings are then assigned to float values\n",
    "\n",
    "import re\n",
    "string_money_df = string_money_df.applymap(lambda x: re.sub(r'[^a-zA-Z0-9 -]', '', str(x)))\n",
    "string_money_df_cleaned = string_money_df.astype(dtype=float)\n",
    "string_money_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop contact and poutcome variables\n",
    "df_clean.drop(df_clean[['contact', 'poutcome']], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows that contain missing values in a specific column\n",
    "\n",
    "adjusted_df = df['Column_With_Missing_Values'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We identify the missing rows from the variable job\n",
    "#these rows were dropped from the entire dataset\n",
    "####Drops all missing rows from job\n",
    "df_clean=df_unclean[df_unclean['job'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes and Groupby\n",
    "\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine multiple dataframes into one dataframe with side by side columns\n",
    "\n",
    "Imputed_dataset = pd.concat([imputed_cat_df, imputed_int_df, imputed_numeric_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging Two Dataframes\n",
    "\n",
    "#This is dataframe includes target variable and all explanatory variables\n",
    "df_merged = pd.merge(df1, df2,  how='left', left_on='GIDBG', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby aggregate on multiple variables\n",
    "\n",
    "#Aggregate vars of interest by merging column and chemical\n",
    "#This gives chemical count for unique latitude and longitudes\n",
    "\n",
    "unique_merging_df = vars_of_interest_df.groupby(['Merging_Column','30. CHEMICAL']).aggregate(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows all variables in a dataframe that have a column name from a list\n",
    "#Just another way of doing it\n",
    "\n",
    "DF_Final_Vars.columns[DF_Final_Vars.columns.isin(cat_var_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.hist(x,bins=None, range=None, density=None, weights=None, cumulative=False, bottom=N\n",
    "                       one, histtype='bar', align='mid', orientation='vertical', rwidth=None, \n",
    "                       log=False, color=None, label=None, stacked=False, normed=None, *, data=None,\n",
    "                       **kwargs)[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(x=df[\"bedrooms\"],y=df[\"price1000\"])\n",
    "plt.ylim(0, 5000)\n",
    "plt.title(\"Number of bedrooms in homes sold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = new_data.corr()\n",
    "plt.figure(figsize=(20,10))\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "plt.title('Variables of Interest in our Analysis', fontsize=40)\n",
    "sns.heatmap(new_data.corr(),annot=True, linewidth=.5, cmap=\"coolwarm\", mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model.resid\n",
    "fig = sm.qqplot(residuals, stats.t, fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We identify the mode of the education variable\n",
    "df_clean['education'].mode()\n",
    "\n",
    "#We impute the missing values of education with its mode value\n",
    "# fill missing values with mean column values\n",
    "df_clean['education'].replace(to_replace = np.nan, value= 'whatever the mode is', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute categorical variables\n",
    "\n",
    "#This is to calculate median for strings\n",
    "#Scipy for calculating mode\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "#Loop\n",
    "#I'm Here is to show if loop breaks\n",
    "for i in range(0, len(cat_var_df.columns)):\n",
    "        print('im here:', cat_var_df.columns[i])\n",
    "        imputed_cat_df.iloc[:,i].replace(np.NaN, scipy.stats.mode(imputed_cat_df.iloc[:,i], axis=0, nan_policy='raise')[0][0], inplace=True)      \n",
    "#impute mode for categorical vars\n",
    "\n",
    "#check\n",
    "imputed_cat_df.isna().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute median for integers\n",
    "imputed_int_df = int_var_df\n",
    "for i in range(0, len(int_var_df.columns)):\n",
    "        imputed_int_df.iloc[:,i].replace(np.NaN, imputed_int_df.iloc[:,i].median(), inplace=True)\n",
    "        \n",
    "#check\n",
    "imputed_int_df.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute mean for numerical vars\n",
    "imputed_numeric_df = numeric_var_df\n",
    "for i in range(0, len(numeric_var_df.columns)):\n",
    "        print('im here:', numeric_var_df.columns[i])\n",
    "        imputed_numeric_df.iloc[:,i].replace(np.NaN, imputed_numeric_df.iloc[:,i].mean(), inplace=True)\n",
    "        \n",
    "##Imputed numerical dataframe\n",
    "\n",
    "#check\n",
    "imputed_numeric_df.isna().sum()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get one hot encoded values for independent categorical variables\n",
    "#merge one hot encoded variables with independent numeric variables\n",
    "\n",
    "X_enc = pd.get_dummies(df_clean[['marital', 'job', 'education', 'default', 'housing', 'loan', 'y']], drop_first= True)\n",
    "\n",
    "numeric_data = df_clean[['age', 'balance', 'duration', 'campaign', 'previous']]\n",
    "    \n",
    "cleaned_data_merge = pd.merge(numeric_data, X_enc, how = 'left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into dependent and independent variables\n",
    "\n",
    "cleaned_data_x = cleaned_data_merge.iloc[:,:-1]\n",
    "cleaned_data_x\n",
    "cleaned_data_y = cleaned_data_merge.iloc [:,-1:]\n",
    "cleaned_data_x.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values for independent variables\n",
    "cleaned_data_x.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into training and testing groups\n",
    "#We will have to standardize the numeric independent variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cleaned_data_x, cleaned_data_y, test_size=0.1, random_state= 1254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate training numeric independent variables\n",
    "x_train_numeric = x_train.iloc[:,:5]\n",
    "\n",
    "\n",
    "#Isolate tersting numeric independent variables\n",
    "x_test_numeric = x_test.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply standard scalar\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "scalar_x_train_numeric = pd.DataFrame(scalar.fit_transform(x_train_numeric), index =x_train_numeric.index, columns=x_train_numeric.columns[0:5], )\n",
    "scalar_x_test_numeric = pd.DataFrame(scalar.transform(x_test_numeric), index = x_test_numeric.index, columns=x_test_numeric.columns[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge standardized numerical independent variables with categorical independent variables\n",
    "x_train_complete = pd.merge(scalar_x_train_numeric, x_train.iloc[:,5:], how = 'left', left_index=True, right_index=True)\n",
    "x_test_complete = pd.merge(scalar_x_test_numeric, x_test.iloc[:,5:], how = 'left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check x training data\n",
    "x_train_complete.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Model\n",
    "X = sm.add_constant(X)# adding a constant\n",
    "model=sm.OLS(Y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin fitting regression model\n",
    "import statsmodels.api as sm\n",
    "import sklearn.linear_model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#create an instance and fit the model \n",
    "logmodel = LogisticRegression()\n",
    "sk_res = logmodel.fit(x_train_complete, y_train)\n",
    "sk_predictions = logmodel.predict(x_test_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# random forest model creation\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train_complete, y_train)\n",
    "# predictions\n",
    "rfc_predict = rfc.predict(x_test_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(sk_res, x_train_complete, y_train, cv=10, scoring='accuracy')\n",
    "print('Cross Validaiton Scores', scores)\n",
    "print('Cross Validation Mean:', sum(scores)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, sk_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-92c4f00daaf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy score for Logistic is: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "print('Accuracy score for Logistic is: '+ str(accuracy_score(y_test, sk_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Requests\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull request from an API example\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://geo.fcc.gov/api/census/block/find'\n",
    "parameters = {\"lat\": latitude_longitude_df['12. LATITUDE'][0], \"lon\": latitude_longitude_df['13. LONGITUDE'][0], 'showall':'false'}\n",
    "req = requests.get(url, parameters)\n",
    "\n",
    "print(req.status_code)\n",
    "print(req.headers)\n",
    "print(req.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop Will pull requests for any set of latitudes and longitudes\n",
    "#Change the length of the loop to go through an entire dataframe\n",
    "#This loop saves a file every 5000 requests\n",
    "#File Names will be labeled like this'FIPs' + '_' + str(count) + '.csv'\n",
    "#Files are updated and overwritten every time the loop is run\n",
    "#If there is an error in the loop SAVE PROGRESS TO A DIFFERENT FILE NAME\n",
    "\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "latitude_list = []\n",
    "longitude_list = []\n",
    "fips_code_list = []\n",
    "timestamps_list = []\n",
    "count = 0 \n",
    "url = 'https://geo.fcc.gov/api/census/block/find'\n",
    "for i in range(0, 5000):\n",
    "    parameters = {\"lat\": latitude_longitude_df['12. LATITUDE'][i], \"lon\": latitude_longitude_df['13. LONGITUDE'][i], 'showall':'false'}\n",
    "    try:\n",
    "        response = requests.get(url, parameters)\n",
    "        data = response.json()        \n",
    "        latitude_list.append(latitude_longitude_df['12. LATITUDE'][i])\n",
    "        longitude_list.append(latitude_longitude_df['13. LONGITUDE'][i])\n",
    "        fips_code_list.append(data['results'][0]['block_fips'])\n",
    "        timestamps_list.append(datetime.datetime.utcnow())\n",
    "    except:\n",
    "        print('Error at index:', i)\n",
    "        fips_code_list.append('Error')\n",
    "    if ((len(fips_code_list) % 5000) == 0):\n",
    "        FIPs_API_df = pd.DataFrame({'Latitude': latitude_list, 'Longitude': longitude_list, 'FIPS': fips_code_list, 'API_Time': timestamps_list})\n",
    "        pd.DataFrame.to_csv(FIPs_API_df, 'FIPs' + '_' + str(count) + '.csv')\n",
    "        count += 1\n",
    "        \n",
    "FIPs_API_df = pd.DataFrame({'Latitude': latitude_list, 'Longitude': longitude_list, 'FIPS': fips_code_list, 'API_Time': timestamps_list})\n",
    "FIPs_API_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Example\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creates awesome correlation matrix using Seaborn\n",
    "# Needs matplotlib and sns\n",
    "\n",
    "corr = new_data.corr()\n",
    "plt.figure(figsize=(20,10))\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "plt.title('Variables of Interest in our Analysis', fontsize=40)\n",
    "sns.heatmap(new_data.corr(),annot=True, linewidth=.5, cmap=\"coolwarm\", mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Great QQ plot for Regression Analysis\n",
    "\n",
    "#Residual Plot\n",
    "residuals = model.resid\n",
    "fig = sm.qqplot(residuals, stats.t, fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into training and testing groups\n",
    "#We will have to standardize the numeric independent variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cleaned_data_x, cleaned_data_y, test_size=0.2, random_state= 1254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Residual vs. Prediction Plot for Regression using Seaborn\n",
    "\n",
    "sns.set(style='ticks')\n",
    "sns.regplot(Y_test, y=predictions, scatter_kws={'alpha':0.05});\n",
    "plt.title('Prediction Performance for House Price, < $1 million, 5-folds')\n",
    "plt.ylabel('Test Data')\n",
    "plt.xlabel('Trained Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Implementation\n",
    "\n",
    "#Data has been prepped\n",
    "#Begin fitting regression model\n",
    "import statsmodels.api as sm\n",
    "import sklearn.linear_model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#create an instance and fit the model \n",
    "logmodel = LogisticRegression()\n",
    "sk_res = logmodel.fit(x_train, y_train)\n",
    "sk_predictions = logmodel.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, sk_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries Example\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a pizza input and dictionary\n",
    "p = int (input (\"Enter the no. of pizzas you want to buy (max 3): \"))\n",
    "t = int (input (\"Enter the toppings you would like in each pizza (max 4): \"))\n",
    "b = 1\n",
    "dict_pizza = {}\n",
    "for _ in range(p):\n",
    "    pizza = \"\"\n",
    "    pizza = str(input(f\"\\nEnter the flavor of Pizza No. {b}: \"))\n",
    "    dict_pizza[pizza] = []\n",
    "    b += 1\n",
    "    c = 1\n",
    "    for _ in range(t):    \n",
    "        topping = str(input(f\"Enter the flavor of Topping No. {c}: \"))\n",
    "        dict_pizza[pizza].append(topping)\n",
    "        c += 1\n",
    "        \n",
    "print(dict_pizza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final\n",
    "#Here is what your program would look like with all the changes\n",
    "\n",
    "import os\n",
    "\n",
    "available_pizzas = ['margarita', 'pollo', '4cheese', 'bolognese', 'vegetarian']\n",
    "available_toppings = ['mushroom', 'onions', 'green pepper', 'extra cheese']\n",
    "pizza_prices = {'margarita': 5, 'pollo': 7, '4cheese': 6, 'bolognese': 8, 'vegetarian': 6.5}\n",
    "topping_prices = {'mushroom':1, 'onions': 2, 'green pepper':3, 'extra cheese':4}\n",
    "\n",
    "def ShowMenu():\n",
    "    os.system('cls')\n",
    "    print(\"Available Pizzas:\\n\")\n",
    "    print(*available_pizzas,sep = ', ')\n",
    "    print(\"\\n\\nAvailable Topings:\\n\")\n",
    "    print(*available_toppings,sep = ', ')\n",
    "    print('\\n\\n')\n",
    "\n",
    "def TakeOrderInput():\n",
    "    os.system('cls')\n",
    "    print(\"Hi, welcome to our text based pizza ordering\")\n",
    "    ordering = True\n",
    "    while ordering:\n",
    "        os.system('cls')\n",
    "        ShowMenu()\n",
    "        pizza = input(\"Please choose a pizza: \")\n",
    "        if pizza not in available_pizzas:\n",
    "            print(f\"I am sorry, we currently do not have {pizza}\\n.\")\n",
    "            os.system('pause')\n",
    "            continue\n",
    "        topping = input(\"Please choose a topping: \")\n",
    "        if topping not in available_toppings:\n",
    "            print(f\"I am sorry, we currently do not have {topping}\\n.\")\n",
    "            os.system('pause')\n",
    "            continue\n",
    "\n",
    "        print(f\"Final order: {pizza} with topping {topping}: \")\n",
    "        ordering = False\n",
    "\n",
    "    return pizza,topping\n",
    "\n",
    "class Order:\n",
    "    def __init__(self):\n",
    "        taxes = 0 #You can add taxes\n",
    "        pizza,topping = TakeOrderInput()\n",
    "        self.type = pizza\n",
    "        self.topping = topping\n",
    "        self.PizzaPrice = pizza_prices[pizza]\n",
    "        self.ToppingPrice = topping_prices[topping]\n",
    "        self.Total = self.PizzaPrice + self.ToppingPrice\n",
    "\n",
    "\n",
    "choice = True\n",
    "orders = []\n",
    "orderchoice = input(\"Welcome! Would you like to order ? (y/n): \")\n",
    "if orderchoice == 'n':\n",
    "    print(\"Have a nice day!\")\n",
    "else:\n",
    "    while choice:\n",
    "        neworder = Order()\n",
    "        orders.append(neworder)\n",
    "        newchoice = input(\"Would you like to order again? (y/n): \")\n",
    "        if (newchoice) == 'n':\n",
    "            break\n",
    "\n",
    "total = 0\n",
    "for order in orders:\n",
    "    total+=order.Total\n",
    "\n",
    "print(\"Total: \",total, '$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and OOP\n",
    "[Go back to the Table of Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(object):\n",
    "    def __init__(self, name, age, gender, level, grades=None):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.gender = gender\n",
    "        self.level = level\n",
    "        self.grades = grades or {}\n",
    "\n",
    "    def setGrade(self, course, grade):\n",
    "        self.grades[course] = grade\n",
    "\n",
    "    def getGrade(self, course):\n",
    "        return self.grades[course]\n",
    "\n",
    "    def getGPA(self):\n",
    "        return sum(self.grades.values())/len(self.grades)\n",
    "\n",
    "# Define some students\n",
    "john = Student(\"John\", 12, \"male\", 6, {\"math\":3.3})\n",
    "jane = Student(\"Jane\", 12, \"female\", 6, {\"math\":3.5})\n",
    "\n",
    "# Now we can get to the grades easily\n",
    "print(john.getGPA())\n",
    "print(jane.getGPA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Inheritance\n",
    "\n",
    "# parent class\n",
    "class Bird:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Bird is ready\")\n",
    "\n",
    "    def whoisThis(self):\n",
    "        print(\"Bird\")\n",
    "\n",
    "    def swim(self):\n",
    "        print(\"Swim faster\")\n",
    "\n",
    "# child class\n",
    "class Penguin(Bird):\n",
    "\n",
    "    def __init__(self):\n",
    "        # call super() function\n",
    "        super().__init__()\n",
    "        print(\"Penguin is ready\")\n",
    "\n",
    "    def whoisThis(self):\n",
    "        print(\"Penguin\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Run faster\")\n",
    "\n",
    "peggy = Penguin()\n",
    "peggy.whoisThis()\n",
    "peggy.swim()\n",
    "peggy.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Creating Dictionaries with string as key and int as value\n",
    "    '''                                  \n",
    "    wordFrequency = {\n",
    "        \"Hello\" : 7,\n",
    "        \"hi\" : 10,\n",
    "        \"there\" : 45,\n",
    "        \"at\" : 23,\n",
    "        \"this\" : 77\n",
    "        }\n",
    "    '''\n",
    "    Iterate over the dictionary using for loop\n",
    "    '''\n",
    "    for key in wordFrequency:\n",
    "        value = wordFrequency[key]\n",
    "        print(key, \" :: \", value)\n",
    "    \n",
    "    print(\"**************\")    \n",
    "    \n",
    "    '''\n",
    "    Iterate over the dictionary using items()\n",
    "    '''    \n",
    "    for key , value in wordFrequency.items():\n",
    "        print(key, \" :: \", value)    \n",
    "    # Take a dictionary view \n",
    "    dictView =  wordFrequency.items()\n",
    "    \n",
    "    print(\"Dictionary View before modification : \", dictView, sep =\"\\n\")\n",
    "    \n",
    "    # Modify the dictionary\n",
    "    wordFrequency[\"hi\"] = 90\n",
    "    \n",
    "    print(\"Dictionary View after modification : \", dictView, sep =\"\\n\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
